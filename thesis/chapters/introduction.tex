%% where does my work fit in?
%% how would I explain my work to a layman?
%% why I chose to work on what I did
%% why these gaps are the ones to be filled (at this time)
%% emphasise methodological contributions
%% mindset:
%% * generalise everything
%% * implement a solution that works well in practice
%% * use both theoretical and experimental methods to understand why it works
%% why the reader should find this work interesting AND how the current state of research led me to work on my RQ
%% why, what, and how

%% motivation (context)
%% (related efforts)
%% problem formulation
%% research objectives
%% research questions
%% contributions

% broad goal -> objectives -> another specific area -> the problem -> specifics

% 5-14 pages (9 on average)
\chapter{Introduction}\label{chapter:introduction}

% ==================== WMC====================

% * description and example(s)

Probabilistic methods are central to artificial intelligence
\citep{DBLP:books/aw/RN2020}, data science \citep{provost2013data}, statistics,
and machine learning \citep{DBLP:books/lib/Bishop07,DBLP:books/daglib/0023091}.
A fundamental task when working with probabilities is to compute the probability
of an event from some description of a probability space, i.e.,
\emph{probabilistic inference}. Examples of probabilistic inference include
computing the partition function of a Markov network or a marginal (or
conditional) probability from a Bayesian network. This thesis is about a
particular approach to probabilistic inference (and other similar computational
tasks) and its use in performing inference on structured representations of
probability distributions such as Bayesian networks and probabilistic logic
programs. More generally, we look at various ways of describing arithmetic
computations pertinent to probabilistic inference and how algorithms interpret
those descriptions, leading to solutions of varying complexity. We begin with an
example that showcases how one can compute a probability in various ways
depending on how one chooses to reason about it.

\begin{example}\label{example:veryfirst}
  Suppose we have a biased coin with probability $0 \le p \le 1$ of landing
  heads. What is the probability that it lands heads \emph{at least once} if we
  toss it \emph{three times}? More formally, we have three independent Bernoulli
  random variables $X_1$, $X_2$, and $X_3$ such that $X_i \sim \Bernoulli(p)$
  for all $i$, and we want to compute
  \[
    P \coloneqq \Pr(X_1 = 1 \cup X_2 = 1 \cup X_3 = 1).
  \]

  The conceptually simplest way of calculating the value of $P$ is by adding
  seven terms, each of which is a product of three factors, i.e., either $p$ or
  $1-p$. This way, we get
  \begin{equation} \label{eq:naive}
    P = ppp + pp(1-p) + \cdots + (1-p)(1-p)p.
  \end{equation}
  One can compute the probability of any event in such a way, although the
  number of arithmetic operations in \cref{eq:naive} scales exponentially with
  the number of variables.

  It is more computationally efficient to reason as follows. If $X_1 = 1$, then
  all combinations of values of $X_2$ and $X_3$ are in the event whose
  probability we are trying to compute. If $X_1 = 0$, then we can similarly
  reason about the value of $X_3$ being immaterial if $X_2 = 1$. This line of
  reasoning gives us the following way to calculate the probability of interest:
  \begin{equation} \label{eq:wmcexample}
    P = p \times 1 \times 1 + (1-p)(p \times 1 + (1-p)p).
  \end{equation}

  Even more efficiently, one can recognize that the only sequence of coin toss
  results \emph{not} in the event $X_1 = 1 \cup X_2 = 1 \cup X_3 = 1$ is
  $X_1 = 0$, $X_2 = 0$, and $X_3 = 0$. Thus, the value of $P$ can be computed as
  \begin{equation} \label{eq:wfomcexample}
    P = 1 - {(1-p)}^3.
  \end{equation}
\end{example}

The first of these three approaches hints at the central problem of this thesis.
Our goal is to efficiently compute a sum-of-products expression such as the one
in \cref{eq:naive}. Of course, the difficulty of this problem partially depends
on how each problem instance is formulated, i.e., the input format. The main
input format that we concern ourselves with is based on propositional
logic---this variation of the problem is known as \emph{weighted model counting}
(WMC) \citep{DBLP:journals/ai/ChaviraD08}. \Cref{eq:wmcexample} is an example of
the kind of efficiency improvements that can be achieved by WMC\@.

Using logic to encode such computational problems may seem like a curious choice
for a reader familiar with probability theory but not logic-based algorithms.
However, propositional logic has long played an important role in efficiently
solving decision, optimisation, and counting problems
\citep{DBLP:series/faia/2009-185}---WMC is just an extension for function
problems. Moreover, WMC has established itself as the state-of-the-art approach
to probabilistic inference across many representations such as probabilistic
programming languages \citep{DBLP:journals/ijar/RiguzziBZCL17} and graphical
models \citep{DBLP:conf/ijcai/AgrawalPM21}.

% * More generally: WMC, WFOMC, and other sum-of-products problems

WMC has been extended in many ways, e.g., to support first-order logic and
continuous variables. The former extension is known as \emph{(symmetric)
  weighted first-order model counting} (WFOMC)
\citep{DBLP:conf/ijcai/BroeckTMDR11}. WFOMC algorithms capitalise on
mathematical operations besides multiplication and addition and thus can compute
$P$ from \cref{example:veryfirst} as in \cref{eq:wfomcexample}. The latter
extension is called \emph{weighted model integration} (WMI)
\citep{DBLP:conf/ijcai/BellePB15}. In WMI, constraints on continuous variables
are described using a fragment of first-order logic known as \emph{linear
  arithmetic over the rationals} (LRA), i.e., inequalities with addition. The
two extensions combined into one are known as \emph{(symmetric) weighted
  first-order model integration} (WFOMI) \citep{DBLP:conf/uai/FeldsteinB21}.

Instead of performing addition and multiplication on numbers, one can do so on
elements of an arbitrary (commutative) semiring. This extension of WMC is known
as \emph{algebraic model counting} (AMC) \citep{DBLP:journals/japll/KimmigBR17}.
AMC has also been extended to support first-order logic---this is known as
\emph{semiring programming} \citep{DBLP:journals/ijar/BelleR20}. Another
important generalisation offered by SP is flexibility in how the numbers that
are to be multiplied and added (i.e., the \emph{weights}) can be defined. In
this thesis, we do something similar within the constraints of a modern
(propositional) WMC algorithm and call our generalisation \emph{pseudo-Boolean
  projection} (PBP).

\begin{table}[t]
  \centering
  \begin{tabular}{lll}
    \toprule
    Problem(s) & Sum/Integral (over) & Product (over) \\
    \midrule
    \rowcolor{gray!10}WMC, AMC & & literals \\
    \rowcolor{gray!10}PBP & \multirow{-2}{*}{models of a propositional theory} & arbitrary \\
    WMI & models of a propositional LRA theory & literals \\
    WFOMI & models of a first-order LRA theory & predicates \\
    \rowcolor{gray!10}WFOMC & & predicates \\
    \rowcolor{gray!10}SP & \multirow{-2}{*}{models of a first-order theory} & arbitrary \\
    SumProd & instantiations of discrete variables & functions \\
    Algebraic path & paths in a graph & edges in a path \\
    Permanent & permutations & elements of a matrix \\
    \bottomrule
  \end{tabular}
  \caption{An assortment of problems that require one to compute a quantity
    defined as a sum of products.}\label{table:comparison}
\end{table}

While WMC and its extensions use logic-based input formats, other
sum-of-products problems have been studied before. For instance, the
\emph{SumProd} problem, which generalises problems such as probabilistic
inference in Bayesian networks and propositional model counting, is defined in
terms of discrete variables and functions
\citep{DBLP:journals/jair/BacchusDP09,DBLP:journals/ai/Dechter99}. In this case,
the sum is over all possible instantiations of the variables, and the product is
over the values of the functions. Another similar problem is the \emph{algebraic
  path problem} where the sum is over all paths in a graph from one node to
another, and the product is over the weights of the edges in the path
\citep{DBLP:series/synthesis/2010Baras}. This problem generalises many graph
problems such as shortest and longest path and has many uses in routing and
network reliability analysis. Lastly, even famous problems in algebraic
complexity theory such as computing the determinant or the permanent of a matrix
are examples of this sum-of-products computational paradigm
\citep{DBLP:books/daglib/0090316,DBLP:journals/tcs/Valiant79}. See
\cref{table:comparison} for a summary of all of the discussed problems and the
work of \citet{DBLP:journals/japll/KimmigBR17} for a more in-depth discussion on
some of them.

% Many reductions are possible among instances of these problems, e.g., both
% WFOMC and SumProd can be reduced to WMC\@.

WMC is a rapidly growing area of research. Publications describing novel WMC
algorithms continue to appear each year
\citep{DBLP:conf/cp/DudekPV20,DBLP:conf/cp/KorhonenJ21}. Furthermore, a
competition\footnote{\url{https://mccompetition.org/}} (as well as a workshop)
for model counting and its extensions started running annually in 2020
\citep{DBLP:journals/jea/FichteHH21}. Given all of this, it is all the more
important to
\begin{itemize}
  \item develop WMC algorithms with good empirical performance,
  \item understand the comparative strengths and weaknesses of different
        approaches,
  \item and optimise the encoding process that transforms problems from the
        application (e.g., probabilistic inference) domain to a representation
        accepted by the algorithm.
\end{itemize}
In this thesis, we contribute to all three of these objectives, particularly
focusing on the semantics of WMC and benchmarking WMC algorithms---two areas
largely neglected by previous work.

\section{The Approach}

% ==================================== A paragraph on the approach: generalisation, main ideas ====================

Our main conceptual tool on this quest is generalisation. While the term
\emph{generalisation} can be defined in many ways, we use it to mean that $x$ is
a generalisation of $y$ if $x$ can do/express/capture everything that $y$ can,
and more. Many important results in science and mathematics are generalisations,
e.g., the Lebesgue integral generalises the Riemann integral, and Einstein's
general theory of relativity generalises Newton's law of universal gravitation.
An example of generalisation closer to home is the emergence of solvers for,
e.g., Boolean satisfiability (SAT), constraint programming, integer programming,
and linear programming, which can be used to solve many decision and
optimisation problems. While designing algorithms for specific problems remains
a valuable enterprise, there is indisputable value in having a range of tools
with broader applicability.

% =========================== A paragraph that partitions the chapters and describes them in broad terms =========

We establish the following generalisations. In \cref{chapter:wmc1,chapter:wmc2},
we generalise the definition of WMC (to PBP) to take full advantage of the
capabilities of recent developments in WMC algorithms. In \cref{chapter:wfomc},
we generalise some of the procedures and data structures used by a WFOMC
algorithm to make it applicable to a wider range of problem instances. The main
idea behind this work is that, in many cases, efficiency improvements can be
achieved by developing algorithms that can handle richer data structures.
Another important idea in this work, particularly in
\cref{chapter:randomlps,chapter:comparison}, is that empirical testing of
algorithms on a wide range of random instances can help reveal fundamental
differences in the behaviour of said algorithms. In
\cref{sec:introcontributions}, we review the contributions and the structure of
this thesis in more detail.

\subsection{Contributions and Outline}\label{sec:introcontributions}

% ==================================== Paragraphs for individual chapters/contributions ===========================

% 1. WMC as a measure on a BA, showing that it's experimentally promising to generalise weight functions
% \Cref{chapter:wmc1} shows how WMC can be seen as the problem of computing the value of a measure on some element of a Boolean algebra.

Encoding a probabilistic inference problem as an instance of WMC typically
necessitates adding extra literals and clauses. This is partly so because the
predominant definition of WMC assigns weights to models based on weights on
literals, and this severely restricts what probability distributions can be
represented. In \cref{chapter:wmc1}, we develop a \emph{measure-theoretic
  perspective on WMC} and propose a way to encode conditional weights on
literals analogously to conditional probabilities. This representation can be as
succinct as standard WMC with weights on literals but can also expand as needed
to represent probability distributions with less structure. To demonstrate the
performance benefits of conditional weights over the addition of extra literals,
we develop a \emph{new WMC encoding} for Bayesian networks and adapt a recent
WMC algorithm \textsc{ADDMC}~\citep{DBLP:conf/aaai/DudekPV20} to the new format.
Our experiments show that the new encoding significantly improves the
performance of the algorithm on most benchmark instances. \Cref{chapter:wmc1} is
published as:
\begin{displayquote}
  \bibentry{DBLP:conf/uai/DilkasB21}
\end{displayquote}

% 2. WMC -> PBP transformation
% 3. several sets of sufficient conditions for its correctness
% 4. evidence that it is experimentally useful
% and using recent state-of-the-art WMC techniques based on pseudo-Boolean function manipulation, competitive with the more traditional WMC algorithms based on knowledge compilation and backtracking search. 

\Cref{chapter:wmc2} builds on \cref{chapter:wmc1} and further considers WMC in
its full generality, leading to the definition of PBP\@. Here we present an
\emph{algorithm that transforms WMC instances into PBP instances} while
eliminating around \SI{43}{\percent} of variables on average across various
Bayesian network encodings. Moreover, we identify \emph{sufficient conditions}
for such a variable removal to be possible. Our experiments show significant
improvement in WMC-based Bayesian network inference. \Cref{chapter:wmc2} is
published as:
\begin{displayquote}
  \bibentry{DBLP:conf/sat/DilkasB21}
\end{displayquote}

% 5. expanded what's liftable using (available) WFOMC algorithms
% 6. showed how a WFOMC algorithm can discover recursive solutions

In \cref{chapter:wfomc}, our attention shifts to another version of WMC, namely,
WFOMC and its unweighted variant FOMC\@. Despite being around for more than a
decade, practical (W)FOMC algorithms are still unable to compute functions as
simple as a factorial. In this chapter, we argue that the capabilities of FOMC
algorithms are severely limited by their inability to express arbitrary
recursive computations. To \emph{enable arbitrary recursion}, we relax the
restrictions that typically accompany domain recursion and generalise circuits
used to express a solution to an FOMC problem to graphs that may contain cycles.
To this end, we enhance the most well-established WFOMC algorithm
\textsc{ForcLift} \citep{DBLP:conf/ijcai/BroeckTMDR11} with new compilation
rules and an algorithm to check whether a recursive call is feasible. These
improvements allow us to \emph{automatically find efficient solutions} to
counting fundamental structures such as injections and bijections.

% 7. developed a way of generating (P)LPs that's much more general than anything from before
% 8. showed the remarkable similarity of various ProbLog inference algorithms when applied to such random data

In \cref{chapter:randomlps,chapter:comparison}, we transition to the other
strand of this work, i.e., random instance generation. Testing algorithms across
a wide range of problem instances is crucial to ensure the validity of any claim
about one algorithm's superiority over another. However, when it comes to
inference algorithms for probabilistic logic programs, experimental evaluations
are limited to only a few programs. Existing methods to generate random logic
programs are limited to propositional programs and often impose stringent
syntactic restrictions. In \cref{chapter:randomlps}, we present a \emph{novel
  approach to generating random logic programs and random probabilistic logic
  programs} using constraint programming, introducing a \emph{new constraint to
  control the independence structure of the underlying probability
  distribution}. We also provide a combinatorial argument for the correctness of
the model, show how the model scales with parameter values, and use the model to
compare probabilistic inference algorithms across a range of synthetic problems.
Our model allows inference algorithm developers to evaluate and compare the
algorithms across a wide range of instances, providing a detailed picture of
their comparative strengths and weaknesses. \Cref{chapter:randomlps} is
published as:
\begin{displayquote}
  \bibentry{DBLP:conf/cp/DilkasB20}
\end{displayquote}

% 9. developed a way to generate propositional formulas in CNF with varying primal treewidth
% 10. showed that ADD-based WMC algorithms scale worse w.r.t. it than all other algorithms

In recent experiments, WMC algorithms are shown to perform similarly overall but
with significant differences on specific subsets of benchmarks. A good
understanding of the differences in the performance of algorithms requires
identifying key characteristics that favour some algorithms over others. In
\cref{chapter:comparison}, we introduce a \emph{random model} for WMC instances
with a parameter that influences primal treewidth---the parameter most commonly
used to characterise the difficulty of an instance. We then use this model to
experimentally compare the performance of WMC algorithms \textsc{c2d}
\citep{DBLP:conf/ecai/Darwiche04}, \textsc{Cachet}
\citep{DBLP:conf/sat/SangBBKP04}, \textsc{d4}
\citep{DBLP:conf/ijcai/LagniezM17}, \textsc{DPMC}
\citep{DBLP:conf/cp/DudekPV20}, and \textsc{miniC2D}
\citep{DBLP:conf/ijcai/OztokD15} on random instances. We show that the
\emph{easy-hard-easy pattern is different} for algorithms based on dynamic
programming and algebraic decision diagrams (ADDs) than for all other solvers.
We also show how all WMC algorithms scale exponentially with respect to primal
treewidth and how this scalability varies across algorithms and densities.
Finally, we demonstrate how the performance of ADD-based algorithms changes
depending on how much determinism or redundancy there is in the numerical values
of weights.

% ============================== Describe the figure ==============================

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[node distance=2.5cm]
    \node[draw,ultra thick,color=gray,text=black] (cp) {Constraint programming};
    \node[draw,ultra thick,right=0.5cm of cp,color=gray,text=black] (random) {Random algorithms};

    \node[draw,ultra thick,color=gray,below=1cm of cp,text=black] (bn) {Bayesian networks};
    \node[draw,ultra thick,right=0.5cm of bn,color=gray,text=black] (problog) {ProbLog};
    \node[draw,ultra thick,right=0.5cm of problog,color=gray,text=black] (mln) {Markov logic};

    \node[draw,ultra thick,below=1cm of bn,color=wmc2,text=black] (pbp) {PBP};
    \node[draw,ultra thick,below=1cm of problog,color=wmc1,text=black] (wmc) {WMC};
    \node[draw,ultra thick,below=1cm of mln,color=gray,text=black] (wfomc) {WFOMC};

    \node[draw,ultra thick,below=1cm of pbp,color=comparison,text=black] (dpmc) {\textsc{DPMC}};
    \node[draw,ultra thick,color=comparison,text=black,left of=dpmc] (addmc) {\textsc{ADDMC}};
    \node[draw,ultra thick,below=1cm of wmc,color=comparison,text=black,text width=1.85cm,align=center] (otherwmc) {Other WMC algorithms};
    \node[draw,ultra thick,below=1cm of wfomc,color=wfomc,text=black] (forclift) {\textsc{ForcLift}};
    \node[draw,ultra thick,right=2.5cm of forclift.north,anchor=north,color=gray,text=black,text width=1.85cm,align=center] (otherwfomc) {Other WFOMC algorithms};

    \draw[-Latex,ultra thick,color=randomlps] (cp) -- (problog);
    \draw[-Latex,ultra thick,color=comparison] (random) to [bend left=20] (wmc);

    \draw[-Latex,ultra thick,color=wmc2] (bn) -- (wmc);
    \draw[-Latex,ultra thick,color=wmc1] (bn) -- (pbp);
    \draw[-Latex,ultra thick,color=gray] (problog) -- (wmc);
    \draw[-Latex,ultra thick,color=gray] (mln) -- (wmc);
    \draw[-Latex,ultra thick,color=gray] (mln) -- (wfomc);

    \draw[-Latex,ultra thick,color=gray] (wfomc) -- (forclift);
    \draw[-Latex,ultra thick,color=gray] (wfomc) -- (otherwfomc);
    \draw[-Latex,ultra thick,color=gray] (wfomc) -- (wmc);
    \draw[-Latex,ultra thick,color=wmc2] (wmc) -- (pbp);
    \draw[-Latex,ultra thick,color=gray] (wmc) -- (addmc);
    \draw[-Latex,ultra thick,color=gray] (wmc) -- (dpmc);
    \draw[-Latex,ultra thick,color=gray] (wmc) -- (otherwmc);
    \draw[-Latex,ultra thick,color=wmc1] (pbp) -- (addmc);
    \draw[-Latex,ultra thick,color=wmc2] (pbp) -- (dpmc);

    \matrix[draw,below left,xshift=1cm] at (current bounding box.north east) {
      \node[fill=wmc1,ultra thick,label=right:\cref{chapter:wmc1}] {}; \\
      \node[fill=wmc2,ultra thick,label=right:\cref{chapter:wmc2}] {}; \\
      \node[fill=wfomc,ultra thick,label=right:\cref{chapter:wfomc}] {}; \\
      \node[fill=randomlps,ultra thick,label=right:\cref{chapter:randomlps}] {}; \\
      \node[fill=comparison,ultra thick,label=right:\cref{chapter:comparison}] {}; \\
    };
  \end{tikzpicture}
  \caption{Concepts relevant to the thesis. The first row contains two
    approaches to generating random problem instances. The second row contains
    some representations of probability distributions. The third row contains
    encodings, i.e., computational problems that encode probabilistic inference
    tasks. The last row contains WMC and WFOMC algorithms. Each chapter is
    assigned a colour that indicates which concepts and interactions between
    concepts the chapter is about.}\label{fig:overview}
\end{figure}

We end the introduction with a visual description of the topics covered in this
thesis. \Cref{fig:overview} lists some of the key concepts of this work and
shows how each chapter of the thesis relates to these concepts and the
interactions between them. \Cref{chapter:background} covers a selection of
topics related to this work and refers the reader to suitable literature for
further information. \Cref{chapter:wmc1} examines the definition of WMC more
closely and suggests a way to bypass it, leading to a more succinct encoding of
Bayesian network probabilistic inference compatible with the \textsc{ADDMC}
algorithm. Then \cref{chapter:wmc2} describes WMC encodings for Bayesian
networks, defines PBP, shows how to transform WMC instances to PBP instances,
and how the \textsc{DPMC} algorithm benefits from this new format. In
\cref{chapter:wfomc}, we expand the capabilities of the WFOMC algorithm
\textsc{ForcLift} to new (previously unsolvable) instances.
\Cref{chapter:randomlps} describes a constraint model that can generate random
(probabilistic) logic programs in the ProbLog \citep{DBLP:conf/ijcai/RaedtKT07}
language---a well-known use-case of WMC\@. \Cref{chapter:comparison} develops an
algorithm for generating random WMC instances and uses it to showcase some
important differences in the behaviour of WMC algorithms. Finally,
\cref{chapter:conclusion} summarises our results and provides a perspective for
potential future work.
