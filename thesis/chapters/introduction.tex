%% where does my work fit in?
%% how would I explain my work to a layman?
%% why I chose to work on what I did
%% why these gaps are the ones to be filled (at this time)
%% emphasise methodological contributions
%% mindset:
%% * generalise everything
%% * implement a solution that works well in practice
%% * use both theoretical and experimental methods to understand why it works
%% why the reader should find this work interesting AND how the current state of research led me to work on my RQ
%% why, what, and how

%% motivation (context)
%% (related efforts)
%% problem formulation
%% research objectives
%% research questions
%% contributions

% broad goal -> objectives -> another specific area -> the problem -> specifics

\chapter{Introduction} % 5-14 pages (9 on average)

% ==================== WMC====================

% * description and example(s)

\begin{example} \label{example:veryfirst}
  Suppose we have a biased coin that has a probability $0 \le p \le 1$ of landing heads. What is the probability that it lands heads \emph{at least once} if we toss it \emph{three times}? More formally, we have three independent Bernoulli random variables $X_1$, $X_2$, and $X_3$ such that $X_i \sim \Bernoulli(p)$ for all $i$, and we want to compute
  \[
  P \coloneqq \Pr(X_1 = 1 \cup X_2 = 1 \cup X_3 = 1).
  \]

  The conceptually simplest way of calculating the value of $P$ is by adding seven terms, each of which is a product of three factors, i.e., either $p$ or $1-p$. This way, we get
  \begin{equation} \label{eq:naive}
  P = ppp + pp(1-p) + \cdots + (1-p)(1-p)p.
  \end{equation}
  One can compute the probability of any event in such a way, although the number of arithmetic operations in \cref{eq:naive} scales exponentially with the number of variables.

  It is more computationally efficient to reason as follows. If $X_1 = 1$, then all combinations of values of $X_2$ and $X_3$ are in the event whose probability we are trying to compute. If $X_1 = 0$, then we can similarly reason about the value of  $X_3$ being immaterial if $X_2 = 1$. This line of reasoning gives us the following way to calculate the probability of interest:
  \begin{equation} \label{eq:wmcexample}
  P = p \times 1 \times 1 + (1-p)(p \times 1 + (1-p)p).
  \end{equation}

  Even more efficiently, one can recognize that the only sequence of coin toss results \emph{not} in the event $X_1 = 1 \cup X_2 = 1 \cup X_3 = 1$ is $X_1 = 0$, $X_2 = 0$, and $X_3 = 0$. Thus, the value of $P$ can be computed as
  \begin{equation} \label{eq:wfomcexample}
  P = 1 - (1-p)^3.
  \end{equation}
\end{example}

The first of these three approaches hints at the central problem of this thesis. Our goal is to efficiently compute a sum-of-products expression such as the one in \cref{eq:naive}. Of course, the difficulty of this problem partially depends on how each problem instance is formulated, i.e., the input format. The main input format that we concern ourselves with is based on propositional logic---this variation of the problem is known as \emph{weighted model counting} (WMC) \citep{DBLP:journals/ai/ChaviraD08}. \Cref{eq:wmcexample} is an example of the kind of efficiency improvements that can be achieved by WMC.

% * More generally: WMC, WFOMC, and other sum-of-products problems

WMC has been extended in many ways, e.g., to support first-order logic and continuous variables. The former extension is known as \emph{(symmetric) weighted first-order model counting} (WFOMC) \citep{DBLP:conf/ijcai/BroeckTMDR11}. WFOMC algorithms capitalise on mathematical operations besides multiplication and addition and thus can compute $P$ from \cref{example:veryfirst} as in \cref{eq:wfomcexample}. The latter extension is called \emph{weighted model integration} (WMI) \citep{DBLP:conf/ijcai/BellePB15}. In WMI, constraints on continuous variables are described using a fragment of first-order logic known as \emph{linear arithmetic over the rationals} (LRA), i.e., inequalities with addition. The two extensions combined into one are known as \emph{(symmetric) weighted first-order model integration} (WFOMI) \citep{DBLP:conf/uai/FeldsteinB21}.

Instead of performing addition and multiplication on numbers, one can do so on elements of an arbitrary (commutative) semiring. This extension of WMC is known as \emph{semiring programming} (SP) \citep{DBLP:journals/ijar/BelleR20}. Another important generalisation offered by SP is flexibility in how the numbers that are to be multiplied and added (i.e., the \emph{weights}) can be defined. In this thesis, we do something similar within the constraints of a modern WMC algorithm and call our generalisation \emph{pseudo-Boolean projection} (PBP).

\begin{table}
  \caption{An assortment of problems that require one to compute a quantity defined as a sum of products}
  \label{table:comparison}
  \centering
  \begin{tabular}{lll}
    \toprule
    Problem & Sum/Integral (over) & Product (over) \\
    \midrule
    WMC & models of a propositional theory & literals \\
    PBP & models of a propositional theory & arbitrary \\
    SP & models of a propositional theory & arbitrary \\
    WMI & models of a propositional LRA theory & literals \\
    WFOMC & models of a first-order theory & predicates \\
    WFOMI & models of a first-order LRA theory & predicates \\
    SumProd & instantiations of discrete variables & functions \\
    Algebraic path & paths in a graph & edges in a path \\
    Permanent & permutations & elements of a matrix \\
    \bottomrule
  \end{tabular}
\end{table}

While WMC and its extensions use logic-based input formats, other sum-of-products problems have been studied before. For instance, the \emph{SumProd} problem, which generalises problems such as probabilistic inference in Bayesian networks and propositional model counting, is defined in terms of discrete variables and functions \citep{DBLP:journals/jair/BacchusDP09,DBLP:journals/ai/Dechter99}. In this case, the sum is over all possible instantiations of the variables, and the product is over the values of the functions. Another similar problem is the \emph{algebraic path problem} where the sum is over all paths in a graph from one node to another, and the product is over the weights of the edges in the path \citep{DBLP:series/synthesis/2010Baras}. This problem generalises many graph problems such as shortest and longest path and has many uses in routing and network reliability analysis. Lastly, even famous problems in algebraic complexity theory such as computing the determinant or the permanent of a matrix are examples of this sum-of-products computational paradigm \citep{DBLP:books/daglib/0090316,DBLP:journals/tcs/Valiant79}. See \cref{table:comparison} for a summary of all of the discussed problems.

Many reductions are possible among instances of these problems, e.g., both WFOMC and SumProd can be reduced to WMC. Empirically, WMC is also a state-of-the-art approach to inference in probabilistic graphical models \citep{DBLP:conf/ijcai/AgrawalPM21}. Publications describing novel WMC algorithms continue to appear each year \citep{DBLP:conf/cp/DudekPV20,DBLP:conf/cp/KorhonenJ21}. Furthermore, a competition\footnote{\url{https://mccompetition.org/}} (as well as a workshop) for model counting and extensions thereof started running annually in 2020 \citep{DBLP:journals/corr/abs-2012-01323}. Given all of this, it is all the more important to
\begin{itemize}
\item develop WMC algorithms with good empirical performance,
\item understand the comparative strengths and weaknesses of different approaches,
\item and optimise the encoding process that transforms problems from the application (e.g., probabilistic inference) domain to a representation accepted by the algorithm.
\end{itemize}

%% Many algorithms for probabilistic inference have been proposed. Variable
%% elimination \citep{DBLP:journals/ai/Dechter99} works by nondeterministically
%% choosing an ordering of variables and `summing out' each variable in the chosen
%% order. The well-known Davis-Putnam algorithm \citep{DBLP:journals/jacm/DavisP60}
%% for SAT is an example of variable elimination
%% \citep{DBLP:journals/jair/BacchusDP09}. Recursive conditioning
%% \citep{DBLP:journals/ai/Darwiche01} is a divide-and-conquer algorithm that relies
%% on a branch decomposition of the hypergraph to initialise variables in such a
%% way to split the problem into multiple smaller problems that can be solved
%% independently. AND/OR search
%% \citep{DBLP:journals/ai/DechterM07,DBLP:books/sp/Nilsson82} is a similar technique
%% that utilises pseudo trees of the primal graph instead of branch decompositions.
%% Belief propagation (also known as message passing) \citep{DBLP:conf/aaai/Pearl82}
%% is an approximation algorithm that converges to the exact answer when the primal
%% graph is a tree, and join (or junction) tree algorithm \citep{lauritzen1988local}
%% extends belief propagation to arbitrary primal graphs via tree decompositions.

\section{Approach, Contributions, and Outline} % 2 pages?

% * A paragraph on the approach: generalisation, main ideas
% TODO: start here

Examples of generalisation
\begin{itemize}
\item SAT/CSP/LP/IP vs algorithms to specific combinatorial problems
\item Einstein (general relativity) vs Newton (gravity)
\item category theory vs algebra/topology/algebraic topology
\item topology vs geometry
\end{itemize}

main ideas:
\begin{itemize}
\item Manipulating more expressive representations can lead to more
  efficient algorithms (c.f., cutting planes vs. resolution in SAT).
\item Random problem instances can help reveal fundamental differences in
  how algorithms behave in practice.
\end{itemize}

Things to mention:
\begin{itemize}
\item reasoning about logic, graphs, and functions
\item In my work, I address this problem by assessing the empirical performance of these algorithms on random instances of different kinds, revealing the weaknesses of the standard definition of WMC, and suggesting more expressive alternatives.
\end{itemize}

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[node distance=2.5cm]
    \node[draw,ultra thick,color=gray,text=black] (cp) {Constraint programming};
    \node[draw,ultra thick,right=0.5cm of cp,color=gray,text=black] (random) {Random algorithms};

    \node[draw,ultra thick,color=gray,below=1cm of cp,text=black] (bn) {Bayesian networks};
    \node[draw,ultra thick,right=0.5cm of bn,color=gray,text=black] (problog) {ProbLog};
    \node[draw,ultra thick,right=0.5cm of problog,color=gray,text=black] (mln) {Markov logic};

    \node[draw,ultra thick,below=1cm of bn,color=wmc2,text=black] (pbp) {PBP};
    \node[draw,ultra thick,below=1cm of problog,color=wmc1,text=black] (wmc) {WMC};
    \node[draw,ultra thick,below=1cm of mln,color=gray,text=black] (wfomc) {WFOMC};

    \node[draw,ultra thick,below=1cm of pbp,color=comparison,text=black] (dpmc) {DPMC};
    \node[draw,ultra thick,color=comparison,text=black,left of=dpmc] (addmc) {ADDMC};
    \node[draw,ultra thick,below=1cm of wmc,color=comparison,text=black,text width=1.85cm,align=center] (otherwmc) {Other WMC algorithms};
    \node[draw,ultra thick,below=1cm of wfomc,color=wfomc,text=black] (forclift) {ForcLift};
    \node[draw,ultra thick,right=2.5cm of forclift.north,anchor=north,color=gray,text=black,text width=1.85cm,align=center] (otherwfomc) {Other WFOMC algorithms};

    \draw[-{Stealth},ultra thick,color=randomlps] (cp) -- (problog);
    \draw[-{Stealth},ultra thick,color=comparison] (random) to [bend left=20] (wmc);

    \draw[-{Stealth},ultra thick,color=wmc2] (bn) -- (wmc);
    \draw[-{Stealth},ultra thick,color=wmc1] (bn) -- (pbp);
    \draw[-{Stealth},ultra thick,color=gray] (problog) -- (wmc);
    \draw[-{Stealth},ultra thick,color=gray] (mln) -- (wmc);
    \draw[-{Stealth},ultra thick,color=gray] (mln) -- (wfomc);

    \draw[-{Stealth},ultra thick,color=gray] (wfomc) -- (forclift);
    \draw[-{Stealth},ultra thick,color=gray] (wfomc) -- (otherwfomc);
    \draw[-{Stealth},ultra thick,color=gray] (wfomc) -- (wmc);
    \draw[-{Stealth},ultra thick,color=wmc2] (wmc) -- (pbp);
    \draw[-{Stealth},ultra thick,color=gray] (wmc) -- (addmc);
    \draw[-{Stealth},ultra thick,color=gray] (wmc) -- (dpmc);
    \draw[-{Stealth},ultra thick,color=gray] (wmc) -- (otherwmc);
    \draw[-{Stealth},ultra thick,color=wmc1] (pbp) -- (addmc);
    \draw[-{Stealth},ultra thick,color=wmc2] (pbp) -- (dpmc);

    \matrix[draw,below left,xshift=1cm] at (current bounding box.north east) {
      \node[fill=wmc1,ultra thick,label=right:\cref{chapter:wmc1}] {}; \\
      \node[fill=wmc2,ultra thick,label=right:\cref{chapter:wmc2}] {}; \\
      \node[fill=wfomc,ultra thick,label=right:\cref{chapter:wfomc}] {}; \\
      \node[fill=randomlps,ultra thick,label=right:\cref{chapter:randomlps}] {}; \\
      \node[fill=comparison,ultra thick,label=right:\cref{chapter:comparison}] {}; \\
    };
  \end{tikzpicture}
  \caption{Concepts relevant to the thesis. The first row contains two approaches to generating random problem instances. The second row contains some representations of probability distributions. The third row contains encodings, i.e., computational problems that encode probabilistic inference tasks. The last row contains WMC and WFOMC algorithms. Each chapter is assigned a colour that indicates which concepts and interactions between concepts the chapter is about.}
  \label{fig:overview}
\end{figure}

% * A paragraph that partitions the chapters and points to the figure.

\Cref{chapter:wmc1,chapter:wmc2,chapter:wfomc} papers address the first idea. There is no reason for weights to only be defined on literals (and there is no reason for a clause to be a just clause).

\Cref{chapter:randomlps,chapter:comparison} undertake the second idea. Random probabilistic logic programs and random 3-CNF formulas.

% * Paragraphs for describing individual contributions.

For each chapter:
\begin{itemize}
\item a short paragraph for each chapter, using the `paragraph' environment to emphasise the contribution
\item Explain how the current state of research led me to work on my RQ. In other words, emphasise timeliness, i.e., why now? Because ADDMC allows for my ideas to become implementable.
\end{itemize}

\begin{displayquote}
  \bibentry{DBLP:conf/uai/DilkasB21}
\end{displayquote}

% definition of WMC is a mistake: convenience rather than reason

% TODO (or with the next paragraph): define / write a bit about pseudo-Boolean functions (necessary for the next chapter)

Weighted model counting (WMC) has emerged as the unifying inference mechanism across many (probabilistic) domains. Encoding an inference problem as an instance of WMC typically necessitates adding extra literals and clauses. This is partly so because the predominant definition of WMC assigns weights to models based on weights on literals, and this severely restricts what probability distributions can be represented. We develop a measure-theoretic perspective on WMC and propose a way to encode conditional weights on literals analogously to conditional probabilities. This representation can be as succinct as standard WMC with weights on literals but can also expand as needed to represent probability distributions with less structure. To demonstrate the performance benefits of conditional weights over the addition of extra literals, we develop a new WMC encoding for Bayesian networks and adapt a state-of-the-art WMC algorithm \textsf{ADDMC} to the new format. Our experiments show that the new encoding significantly improves the performance of the algorithm on most benchmark instances.

\begin{displayquote}
  \bibentry{DBLP:conf/sat/DilkasB21}
\end{displayquote}

Weighted model counting (WMC) is a powerful computational technique for a variety of problems, especially commonly used for probabilistic inference. However, the standard definition of WMC that puts weights on literals often necessitates WMC encodings to include additional variables and clauses just so each weight can be attached to a literal. This paper complements previous work by considering WMC instances in their full generality and using recent state-of-the-art WMC techniques based on pseudo-Boolean function manipulation, competitive with the more traditional WMC algorithms based on knowledge compilation and backtracking search. We present an algorithm that transforms WMC instances into a format based on pseudo-Boolean functions while eliminating around \SI{43}{\percent} of variables on average across various Bayesian network encodings. Moreover, we identify sufficient conditions for such a variable removal to be possible. Our experiments show significant improvement in WMC-based Bayesian network inference, outperforming the current state of the art.

First-order model counting (FOMC) is a \#\P-complete computational problem that asks to count the models of a sentence in first-order logic. Despite being around for more than a decade, practical FOMC algorithms are still unable to compute functions as simple as a factorial. We argue that the capabilities of FOMC algorithms are severely limited by their inability to express arbitrary recursive computations. To enable arbitrary recursion, we relax the restrictions that typically accompany domain recursion and generalise circuits used to express a solution to an FOMC problem to graphs that may contain cycles. To this end, we enhance the most well-established (weighted) FOMC algorithm ForcLift with new compilation rules and an algorithm to check whether a recursive call is feasible. These improvements allow us to find efficient solutions to counting fundamental structures such as injections and bijections.

\begin{displayquote}
  \bibentry{DBLP:conf/cp/DilkasB20}
\end{displayquote}

Testing algorithms across a wide range of problem instances is crucial to ensure the validity of any claim about one algorithm's superiority over another. However, when it comes to inference algorithms for probabilistic logic programs, experimental evaluations are limited to only a few programs. Existing methods to generate random logic programs are limited to propositional programs and often impose stringent syntactic restrictions. We present a novel approach to generating random logic programs and random probabilistic logic programs using constraint programming, introducing a new constraint to control the independence structure of the underlying probability distribution. We also provide a combinatorial argument for the correctness of the model, show how the model scales with parameter values, and use the model to compare probabilistic inference algorithms across a range of synthetic problems. Our model allows inference algorithm developers to evaluate and compare the algorithms across a wide range of instances, providing a detailed picture of their (comparative) strengths and weaknesses.

Weighted model counting (\textsf{WMC}) is an extension of propositional model counting with applications to probabilistic inference and other areas of artificial intelligence. In recent experiments, \textsf{WMC} algorithms are shown to perform similarly overall but with significant differences on specific subsets of benchmarks. A good understanding of the differences in the performance of algorithms requires identifying key characteristics that favour some algorithms over others. In this paper, we introduce a random model for \textsf{WMC} instances with a parameter that influences primal treewidth---the parameter most commonly used to characterise the difficulty of an instance. We then use this model to experimentally compare the performance of \textsf{WMC} algorithms \textsc{c2d}, \textsc{Cachet}, \textsc{d4}, \textsc{DPMC}, and \textsc{miniC2D} on random instances. We show that the easy-hard-easy pattern is different for algorithms based on dynamic programming and algebraic decision diagrams (ADDs) than for all other solvers. We also show how all \textsf{WMC} algorithms scale exponentially with respect to primal treewidth and how this scalability varies across algorithms and densities. Finally, we demonstrate how the performance of ADD-based algorithms changes depending on how much determinism or redundancy there is in the numerical values of weights.
