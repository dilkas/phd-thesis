%%%%
%% Load the class. Put any options that you want here (see the documentation
%% for the list of options). The following are samples for each type of
%% thesis:
%%
%% Note: you can also specify any of the following options:
%%  logo: put a University of Edinburgh logo onto the title page
%%  frontabs: put the abstract onto the title page
%%  deptreport: produce a title page that fits into a Computer Science
%%      departmental cover [not sure if this actually works]
%%  singlespacing, fullspacing, doublespacing: choose line spacing
%%  oneside, twoside: specify a one-sided or two-sided thesis
%%  10pt, 11pt, 12pt: choose a font size
%%  centrechapter, leftchapter, rightchapter: alignment of chapter headings
%%  sansheadings, normalheadings: headings and captions in sans-serif
%%      (default) or in the same font as the rest of the thesis
%%  [no]listsintoc: put list of figures/tables in table of contents (default:
%%      not)
%%  romanprepages, plainprepages: number the preliminary pages with Roman
%%      numerals (default) or consecutively with the rest of the thesis
%%  parskip: don't indent paragraphs, put a blank line between instead
%%  abbrevs: define a list of useful abbreviations (see documentation)
%%  draft: produce a single-spaced, double-sided thesis with narrow margins
%%
%% For a PhD thesis -- you must also specify a research institute:
\documentclass[phd,aiai,twoside,fullspacing,logo]{infthesis}

\usepackage{hyperref}
\usepackage[table]{xcolor}
\usepackage[ruled,vlined,linesnumbered,algochapter]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage[nounderscore]{syntax}
\usepackage{blkarray}
\usepackage{siunitx}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{capt-of}
\usepackage{booktabs}
\usepackage[misc,geometry]{ifsym}
\usepackage{breakcites}
\usepackage[british]{babel}
\usepackage{complexity}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{sectsty}
\usepackage{stmaryrd}
\usepackage{rotating}
\usepackage{pifont}
\usepackage{listings}
\usepackage{microtype}
\usepackage{csquotes}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage[color=lightgray]{todonotes} % TODO: temp

\usepackage{natbib}
\usepackage{bibentry}
\bibliographystyle{abbrvnat}

\allsectionsfont{\raggedright}

\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{calc}
\usetikzlibrary{cd}
\usetikzlibrary{fit}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes}
\usetikzlibrary{trees}

\captionsetup[subfigure]{width=\linewidth}

\newtheorem{assumption}{Assumption}
\newtheorem{conjecture}{Conjecture}
\newtheorem{constraint}{Constraint}
\newtheorem{fact}{Fact}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{experiment}{Experiment}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\numberwithin{assumption}{chapter}
\numberwithin{conjecture}{chapter}
\numberwithin{constraint}{chapter}
\numberwithin{fact}{chapter}
\numberwithin{proposition}{chapter}
\numberwithin{theorem}{chapter}
\numberwithin{lemma}{chapter}
\numberwithin{definition}{chapter}
\numberwithin{example}{chapter}
\numberwithin{experiment}{chapter}

\renewcommand\fbox{\fcolorbox{red}{white}}
\newcommand{\hilight}[1]{\setlength{\fboxsep}{1pt}\colorbox{lightgray}{#1}}
\newcommand{\hlitem}{\stepcounter{enumi}\item[\hilight{\theenumi}]}

\newcommand{\logical}[1]{{\normalfont \texttt{#1}}}
\newcommand{\variable}[1]{\texttt{\textup{#1}}}
\newcommand{\arrayd}[3]{\variable{{#1}[}{#2}\variable{]} \in {#3}}
% 1=name, 2=length, 3=type
\newcommand{\arrayt}[3]{\variable{{#3}} : \variable{{#1}[}{#2}\variable{]}}

\newcommand{\predicates}{\mathcal{P}}
\newcommand{\variables}{\mathcal{V}}
\newcommand{\constants}{\mathcal{C}}
\newcommand{\tokens}{\mathcal{T}}
\newcommand{\arities}{\mathcal{A}}
\newcommand{\maxArity}{\mathcal{M}_{\mathcal{A}}}
\newcommand{\maxNumNodes}{\mathcal{M}_{\mathcal{N}}}
\newcommand{\maxNumClauses}{\mathcal{M}_{\mathcal{C}}}

\newcommand{\true}{\texttt{true}}
\newcommand{\false}{\texttt{false}}
\newcommand{\undefined}{\texttt{undefined}}

\newcommand{\FOtwo}{$\mathsf{FO}^{2}$}
\newcommand{\FOthree}{$\mathsf{FO}^{3}$}
\newcommand{\SFO}{$\mathsf{S}^{2}\mathsf{FO}^{2}$}
\newcommand{\SRU}{$\mathsf{S}^{2}\mathsf{RU}$}
\newcommand{\Uone}{$\mathsf{U}_{1}$}
\newcommand{\Ctwo}{$\mathsf{C}^{2}$}

\newcommand{\mc}{\#{SAT}}

\DeclareMathOperator{\alldifferent}{\mathtt{alldifferent}}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\Determined}{\Delta}
\DeclareMathOperator{\Undetermined}{\Upsilon}
\DeclareMathOperator{\AlmostDetermined}{\Gamma}
\DeclareMathOperator{\CR}{\textsc{CR}}
\DeclareMathOperator{\GDR}{\textsc{GDR}}
\DeclareMathOperator{\IE}{\textsc{IE}}
\DeclareMathOperator{\Reff}{\textsc{Ref}}
\DeclareMathOperator{\wwp}{w}
\DeclareMathOperator{\wwn}{\overline{w}}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Imm}{Im}
\DeclareMathOperator{\Doms}{Doms}
\DeclareMathOperator{\size}{\sigma}
\DeclareMathOperator{\Vars}{Vars}
\DeclareMathOperator{\WMC}{WMC}
\DeclareMathOperator{\gr}{gr}
\DeclareMathOperator{\leftlsquigarrow}{\text{\reflectbox{$\rightsquigarrow$}}}

\Crefname{algocf}{Algorithm}{Algorithms}
\Crefname{constraint}{Constraint}{Constraints}
\Crefname{experiment}{Experiment}{Experiments}
\Crefname{clause}{Clause}{Clauses}
\crefname{line}{line}{lines}

\sisetup{range-phrase=--}
\sisetup{range-units=single}
\newcommand{\crefrangeconjunction}{--}

\makeatletter
\newcommand{\nosemic}{\renewcommand{\@endalgocfline}{\relax}}% Drop semi-colon ;
\newcommand{\dosemic}{\renewcommand{\@endalgocfline}{\algocf@endline}}% Reinstate semi-colon ;
\newcommand{\pushline}{\Indp}% Indent
\newcommand{\popline}{\Indm\dosemic}% Undent
\makeatother

\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
               {\renewcommand\theinnercustomthm{#1}\innercustomthm}
               {\endinnercustomthm}
               \newtheorem{innercustomlemma}{Lemma}
               \newenvironment{customlemma}[1]
                              {\renewcommand\theinnercustomlemma{#1}\innercustomlemma}
                              {\endinnercustomlemma}

\makeatletter
\newcommand\incircbin
    {%
      \mathpalette\@incircbin
    }
    \newcommand\@incircbin[2]
                          {%
                            \mathbin%
                                {%
                                  \ooalign{\hidewidth$#1#2$\hidewidth\crcr$#1\bigcirc$}%
                                }%
                          }
                          \newcommand{\oland}{\incircbin{\land}}
                          \newcommand{\olor}{\incircbin{\lor}}
                          \newcommand{\Contradiction}{\incircbin{\bot}}
                          \newcommand{\Tautology}{\incircbin{\top}}
                          \newcommand{\Smoothing}{\incircbin{}}
                          \newcommand{\Unit}{\incircbin{1}}
                          \makeatother

\newcommand\pfun{\mathrel{\ooalign{\hfil$\mapstochar\mkern5mu$\hfil\cr$\to$\cr}}}
\newcommand\mdoubleplus{\mathbin{+\mkern-10mu+}}
\newcommand*{\twoheadrightarrowtail}{\mathrel{\rightarrowtail\kern-1.9ex\twoheadrightarrow}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\crefname{enumi}{Condition}{Conditions}
\crefname{enumii}{Condition}{Conditions}

\lstset{
  basicstyle=\ttfamily,
  keepspaces=true
}

\definecolor{color1}{HTML}{1b9e77}
\definecolor{color2}{HTML}{d95f02}
\definecolor{color3}{HTML}{7570b3}

\definecolor{wmc1}{HTML}{1b9e77}
\definecolor{wmc2}{HTML}{d95f02}
\definecolor{wfomc}{HTML}{7570b3}
\definecolor{randomlps}{HTML}{e7298a}
\definecolor{comparison}{HTML}{66a61e}

\title{Generalising Weighted Model Counting}
\author{Paulius Dilkas}

% 1-2 (usually 1) pages, <= 300 words (soft constraint)
% 1. problem: description of the problem
% 2. context: (same paragraph) mention the applicability of the studied problems (i.e., many areas of impact within and beyond CS)
% 3. solution: contributions
% 4. validation: evaluation (e.g., where the instances come from) (maybe: new possibilities opened up by my work)

\abstract{Given a formula in propositional or first-order logic and some
  non-negative weights, weighted model counting (WMC) is a function problem that
  asks to compute the sum of the weights of the models of the formula.
  Originally used as a flexible way of performing probabilistic inference on
  graphical models, WMC has found many applications across artificial
  intelligence (AI), machine learning, and other domains. Areas of AI that rely
  on WMC include explainable AI, neural-symbolic AI, probabilistic programming,
  and statistical relational AI\@. WMC also has applications in bioinformatics,
  data mining, natural language processing, prognostics, and robotics.

  In this work, we are interested in revisiting the foundations of WMC and
  considering generalisations of some of the key definitions in the interest of
  conceptual clarity and practical efficiency. We begin by developing a
  measure-theoretic perspective on WMC, which suggests a new and more general
  way of defining the weights of an instance. This new representation can be as
  succinct as standard WMC but can also expand as needed to represent
  less-structured probability distributions. We demonstrate the performance
  benefits of the new format by developing a novel WMC encoding for Bayesian
  networks. We then show how existing WMC encodings for Bayesian networks can be
  transformed into this more general format and what conditions ensure that the
  transformation is correct (i.e., preserves the answer). Combining the
  strengths of the more flexible representation with the tricks used in existing
  encodings yields further efficiency improvements in Bayesian network
  probabilistic inference.

  Next, we turn our attention to the first-order setting. Here, we argue that
  the capabilities of practical model counting algorithms are severely limited
  by their inability to perform arbitrary recursive computations. To enable
  arbitrary recursion, we relax the restrictions that typically accompany domain
  recursion and generalise circuits (used to express a solution to a model
  counting problem) to graphs that are allowed to have cycles. These
  improvements enable us to find efficient solutions to counting fundamental
  structures such as injections and bijections that were previously unsolvable
  by any available algorithm.

  The second strand of this work is concerned with synthetic data generation.
  Testing algorithms across a wide range of problem instances is crucial to
  ensure the validity of any claim about one algorithm's superiority over
  another. However, benchmarks are often limited and fail to reveal differences
  among the algorithms. First, we show how random instances of probabilistic
  logic programs (that typically use WMC algorithms for inference) can be
  generated using constraint programming. We also introduce a new constraint to
  control the independence structure of the underlying probability distribution
  and provide a combinatorial argument for the correctness of the constraint
  model. This model allows us to, for the first time, experimentally investigate
  inference algorithms on more than just a handful of instances. Second, we
  introduce a random model for WMC instances with a parameter that influences
  primal treewidth---the parameter most commonly used to characterise the
  difficulty of an instance. We show that the easy-hard-easy pattern with
  respect to clause density is different for algorithms based on dynamic
  programming and algebraic decision diagrams than for all other solvers. We
  also demonstrate that all WMC algorithms scale exponentially with respect to
  primal treewidth, although at differing rates.}

\begin{document}
\nobibliography*

\begin{preliminary}

\maketitle

\begin{laysummary}
  Given a task to compute a number (e.g., some probability), not all approaches
  are equally efficient. For example, multiplying two integers is likely to be
  faster than simulating multiplication via repeated addition. Moreover, this
  computational task is given to us in some kind of format, and this format
  determines whether an efficient solution is easy to find. Weighted model
  counting (WMC) is an approach to computing sums of products efficiently by
  using logic to describe what needs to be computed. WMC is heavily used in
  artificial intelligence, machine learning, robotics, and many other fields.

  This thesis approaches WMC from two fronts. First, we focus on improving WMC
  algorithms by generalising some of the key ideas and making them more flexible
  and powerful. To this end, we revisit the foundations of WMC and develop new
  input formats that connect logic with numbers (which need to be added and/or
  multiplied) in a more flexible way. We also improve the ability of a WMC
  algorithm to solve subproblems by recognizing them as variations of
  subproblems encountered before. Second, we improve our understanding of the
  differences among WMC algorithms by generating a variety of problem instances
  and testing the algorithms on them. For instances written in a more complex
  format, we find that all WMC algorithms behave extremely similarly, as the
  computational bottleneck seems to happen before WMC even takes place. On the
  other hand, for instances written in a simpler format, we show important
  differences in the performance characteristics of WMC algorithms depending on
  several key parameters that are used to describe these instances.
\end{laysummary}

\begin{acknowledgements}
  % The first author was supported by the EPSRC Centre for Doctoral Training in
  % Robotics and Autonomous Systems, funded by the UK Engineering and Physical
  % Sciences Research Council (grant EP/L016834/1).

  % This work has made use of the resources provided by the Edinburgh Compute and
  % Data Facility (ECDF) (\url{http://www.ecdf.ed.ac.uk/}).

  % We thank the anonymous reviewers for their helpful comments.

  % must acknowledge Timothy van Bremen for helpful discussions (and maybe other people I've met at conferences, e.g., Adnan Darwiche, Jeffrey Dudek)

  % examiners

  % supervisor

  % Fazl, Jonathan and other students supervised by the same supervisor

  % communities: AIAI and Edinburgh Centre for Robotics
\end{acknowledgements}

\standarddeclaration

%% Finally, a dedication (this is optional -- uncomment the following line if
%% you want one).
% \dedication{To my mummy.}

\tableofcontents

%% If you want a list of figures or tables, uncomment the appropriate line(s)
% \listoffigures
% \listoftables

\end{preliminary}

% TODO:
% * !!! incorporate new feedback in all content chapters starting with Chapter 4
% * Add additional information from phd_notes and previous versions of some of the papers (e.g., the section of paper1 in wmc-without-parameters that got deleted in paper2).
% * capitalisation and font for variables (uppercase), constants (lowercase), etc.
% * consider using listings for all Prolog and maybe ProbLog code (but then everything becomes sf instead of tt)
% * make all the arrows in tikz pictures (and tree sibling and node distances) all the same
% * notation for graphs: \mathcal{V}, \mathcal{E}, \mathcal{L}
% * introduction, background, recursion: merge table cells with the same info
% * eliminate warnings (e.g., overflow, single-spacing)
% * background: wmc should be defined at the very beginning
% * introduction: define / write a bit about pseudo-Boolean functions (necessary for the next chapter). Maybe make a connection with pseudo-Boolean constraints.
% * the tikzpicture in the recursion chapter could also be redrawn to be a tree
% * in algorithms, write forall if selecting more than one variable
% * Chapter 5 (and maybe elsewhere): tikzcd diagrams should not be referred to as equations
% * recursion chapter: double check the results in the table
% * write acknowledgements
% * make sure to use \text{, } and \text{. } for FOL
% * V: again make sure that the conclusions are befitting a thesis by indicating pointers to previous and upcoming chapters.
% * be consistent with the capitalisation of paragraphs and example/theorem/etc. names
% * problog shouldn't be sc
% * the spelling of $i$th vs $i$-th

\include{chapters/introduction} % 5-14 pages (9 on average)
\include{chapters/background} % 11-45 pages (27 on average)
\include{chapters/wmc_for_bns/chapter}
\include{chapters/wmc_without_parameters/chapter}
\include{chapters/recursion}
\include{chapters/random_lps/chapter}
\include{chapters/comparison/chapter}
\include{chapters/conclusion} % 5-28 pages (mean: 8, median: 5)
\appendix
\include{chapters/appendix.tex}

%% If you want the bibliography single-spaced (which is allowed), uncomment
%% the next line.
\singlespace

\bibliography{thesis}
%\printbibliography

\end{document}
