%%%%
%% Load the class. Put any options that you want here (see the documentation
%% for the list of options). The following are samples for each type of
%% thesis:
%%
%% Note: you can also specify any of the following options:
%%  logo: put a University of Edinburgh logo onto the title page
%%  frontabs: put the abstract onto the title page
%%  deptreport: produce a title page that fits into a Computer Science
%%      departmental cover [not sure if this actually works]
%%  singlespacing, fullspacing, doublespacing: choose line spacing
%%  oneside, twoside: specify a one-sided or two-sided thesis
%%  10pt, 11pt, 12pt: choose a font size
%%  centrechapter, leftchapter, rightchapter: alignment of chapter headings
%%  sansheadings, normalheadings: headings and captions in sans-serif
%%      (default) or in the same font as the rest of the thesis
%%  [no]listsintoc: put list of figures/tables in table of contents (default:
%%      not)
%%  romanprepages, plainprepages: number the preliminary pages with Roman
%%      numerals (default) or consecutively with the rest of the thesis
%%  parskip: don't indent paragraphs, put a blank line between instead
%%  abbrevs: define a list of useful abbreviations (see documentation)
%%  draft: produce a single-spaced, double-sided thesis with narrow margins
%%
%% For a PhD thesis -- you must also specify a research institute:
\documentclass[phd,aiai,twoside,fullspacing,logo]{infthesis}

\usepackage{hyperref}
\usepackage[table]{xcolor}
\usepackage[ruled,vlined,linesnumbered,algochapter]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage[nounderscore]{syntax}
\usepackage{blkarray}
\usepackage{siunitx}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{capt-of}
\usepackage{booktabs}
\usepackage[misc,geometry]{ifsym}
\usepackage{breakcites}
\usepackage[british]{babel}
\usepackage{complexity}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{sectsty}
\usepackage{stmaryrd}
\usepackage{rotating}
\usepackage{pifont}
\usepackage{listings}
\usepackage{microtype}
\usepackage{csquotes}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage[color=lightgray]{todonotes} % TODO: temp

\usepackage{natbib}
\usepackage{bibentry}
\bibliographystyle{abbrvnat}

\allsectionsfont{\raggedright}

\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{calc}
\usetikzlibrary{cd}
\usetikzlibrary{fit}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes}
\usetikzlibrary{trees}

\captionsetup[subfigure]{width=\linewidth}

\newtheorem{assumption}{Assumption}
\newtheorem{conjecture}{Conjecture}
\newtheorem{constraint}{Constraint}
\newtheorem{fact}{Fact}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{experiment}{Experiment}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\numberwithin{assumption}{chapter}
\numberwithin{conjecture}{chapter}
\numberwithin{constraint}{chapter}
\numberwithin{fact}{chapter}
\numberwithin{proposition}{chapter}
\numberwithin{theorem}{chapter}
\numberwithin{lemma}{chapter}
\numberwithin{definition}{chapter}
\numberwithin{example}{chapter}
\numberwithin{experiment}{chapter}

\renewcommand\fbox{\fcolorbox{red}{white}}
\newcommand{\hilight}[1]{\setlength{\fboxsep}{1pt}\colorbox{lightgray}{#1}}
\newcommand{\hlitem}{\stepcounter{enumi}\item[\hilight{\theenumi}]}

\newcommand{\logical}[1]{{\normalfont \texttt{#1}}}
\newcommand{\variable}[1]{\texttt{\textup{#1}}}
\newcommand{\arrayd}[3]{\variable{{#1}[}{#2}\variable{]} \in {#3}}
% 1=name, 2=length, 3=type
\newcommand{\arrayt}[3]{\variable{{#3}} : \variable{{#1}[}{#2}\variable{]}}

\newcommand{\predicates}{\mathcal{P}}
\newcommand{\variables}{\mathcal{V}}
\newcommand{\constants}{\mathcal{C}}
\newcommand{\tokens}{\mathcal{T}}
\newcommand{\arities}{\mathcal{A}}
\newcommand{\maxArity}{\mathcal{M}_{\mathcal{A}}}
\newcommand{\maxNumNodes}{\mathcal{M}_{\mathcal{N}}}
\newcommand{\maxNumClauses}{\mathcal{M}_{\mathcal{C}}}

\newcommand{\true}{\texttt{true}}
\newcommand{\false}{\texttt{false}}
\newcommand{\undefined}{\texttt{undefined}}

\newcommand{\FOtwo}{$\mathsf{FO}^{2}$}
\newcommand{\FOthree}{$\mathsf{FO}^{3}$}
\newcommand{\SFO}{$\mathsf{S}^{2}\mathsf{FO}^{2}$}
\newcommand{\SRU}{$\mathsf{S}^{2}\mathsf{RU}$}
\newcommand{\Uone}{$\mathsf{U}_{1}$}
\newcommand{\Ctwo}{$\mathsf{C}^{2}$}

\newcommand{\mc}{\#{SAT}}

\DeclareMathOperator{\alldifferent}{\mathtt{alldifferent}}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\Determined}{\Delta}
\DeclareMathOperator{\Undetermined}{\Upsilon}
\DeclareMathOperator{\AlmostDetermined}{\Gamma}
\DeclareMathOperator{\CR}{\textsc{CR}}
\DeclareMathOperator{\GDR}{\textsc{GDR}}
\DeclareMathOperator{\IE}{\textsc{IE}}
\DeclareMathOperator{\Reff}{\textsc{Ref}}
\DeclareMathOperator{\wwp}{w}
\DeclareMathOperator{\wwn}{\overline{w}}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Imm}{Im}
\DeclareMathOperator{\Doms}{Doms}
\DeclareMathOperator{\size}{\sigma}
\DeclareMathOperator{\Vars}{Vars}
\DeclareMathOperator{\WMC}{WMC}
\DeclareMathOperator{\gr}{gr}
\DeclareMathOperator{\leftlsquigarrow}{\text{\reflectbox{$\rightsquigarrow$}}}

\Crefname{algocf}{Algorithm}{Algorithms}
\Crefname{constraint}{Constraint}{Constraints}
\Crefname{experiment}{Experiment}{Experiments}
\Crefname{clause}{Clause}{Clauses}
\Crefname{diagram}{Diagram}{Diagrams}
\crefname{line}{line}{lines}

\sisetup{range-phrase=--}
\sisetup{range-units=single}
\newcommand{\crefrangeconjunction}{--}

\makeatletter
\newcommand{\nosemic}{\renewcommand{\@endalgocfline}{\relax}}% Drop semi-colon ;
\newcommand{\dosemic}{\renewcommand{\@endalgocfline}{\algocf@endline}}% Reinstate semi-colon ;
\newcommand{\pushline}{\Indp}% Indent
\newcommand{\popline}{\Indm\dosemic}% Undent
\makeatother

\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
               {\renewcommand\theinnercustomthm{#1}\innercustomthm}
               {\endinnercustomthm}
               \newtheorem{innercustomlemma}{Lemma}
               \newenvironment{customlemma}[1]
                              {\renewcommand\theinnercustomlemma{#1}\innercustomlemma}
                              {\endinnercustomlemma}

\makeatletter
\newcommand\incircbin
    {%
      \mathpalette\@incircbin
    }
    \newcommand\@incircbin[2]
                          {%
                            \mathbin%
                                {%
                                  \ooalign{\hidewidth$#1#2$\hidewidth\crcr$#1\bigcirc$}%
                                }%
                          }
                          \newcommand{\oland}{\incircbin{\land}}
                          \newcommand{\olor}{\incircbin{\lor}}
                          \newcommand{\Contradiction}{\incircbin{\bot}}
                          \newcommand{\Tautology}{\incircbin{\top}}
                          \newcommand{\Smoothing}{\incircbin{}}
                          \newcommand{\Unit}{\incircbin{1}}
                          \makeatother

\newcommand\pfun{\mathrel{\ooalign{\hfil$\mapstochar\mkern5mu$\hfil\cr$\to$\cr}}}
\newcommand\mdoubleplus{\mathbin{+\mkern-10mu+}}
\newcommand*{\twoheadrightarrowtail}{\mathrel{\rightarrowtail\kern-1.9ex\twoheadrightarrow}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\crefname{enumi}{Condition}{Conditions}
\crefname{enumii}{Condition}{Conditions}

\lstset{
  basicstyle=\ttfamily,
  keepspaces=true
}

\definecolor{color1}{HTML}{1b9e77}
\definecolor{color2}{HTML}{d95f02}
\definecolor{color3}{HTML}{7570b3}

\definecolor{wmc1}{HTML}{1b9e77}
\definecolor{wmc2}{HTML}{d95f02}
\definecolor{wfomc}{HTML}{7570b3}
\definecolor{randomlps}{HTML}{e7298a}
\definecolor{comparison}{HTML}{66a61e}

\title{Generalising Weighted Model Counting}
\author{Paulius Dilkas}

% 1-2 (usually 1) pages, <= 300 words (soft constraint)
% 1. problem: description of the problem
% 2. context: (same paragraph) mention the applicability of the studied problems (i.e., many areas of impact within and beyond CS)
% 3. solution: contributions
% 4. validation: evaluation (e.g., where the instances come from) (maybe: new possibilities opened up by my work)

\abstract{Given a formula in propositional or (finite-domain) first-order logic
  and some non-negative weights, weighted model counting (WMC) is a function
  problem that asks to compute the sum of the weights of the models of the
  formula. Originally used as a flexible way of performing probabilistic
  inference on graphical models, WMC has found many applications across
  artificial intelligence (AI), machine learning, and other domains. Areas of AI
  that rely on WMC include explainable AI, neural-symbolic AI, probabilistic
  programming, and statistical relational AI\@. WMC also has applications in
  bioinformatics, data mining, natural language processing, prognostics, and
  robotics.

  In this work, we are interested in revisiting the foundations of WMC and
  considering generalisations of some of the key definitions in the interest of
  conceptual clarity and practical efficiency. We begin by developing a
  measure-theoretic perspective on WMC, which suggests a new and more general
  way of defining the weights of an instance. This new representation can be as
  succinct as standard WMC but can also expand as needed to represent
  less-structured probability distributions. We demonstrate the performance
  benefits of the new format by developing a novel WMC encoding for Bayesian
  networks. We then show how existing WMC encodings for Bayesian networks can be
  transformed into this more general format and what conditions ensure that the
  transformation is correct (i.e., preserves the answer). Combining the
  strengths of the more flexible representation with the tricks used in existing
  encodings yields further efficiency improvements in Bayesian network
  probabilistic inference.

  Next, we turn our attention to the first-order setting. Here, we argue that
  the capabilities of practical model counting algorithms are severely limited
  by their inability to perform arbitrary recursive computations. To enable
  arbitrary recursion, we relax the restrictions that typically accompany domain
  recursion and generalise circuits (used to express a solution to a model
  counting problem) to graphs that are allowed to have cycles. These
  improvements enable us to find efficient solutions to counting fundamental
  structures such as injections and bijections that were previously unsolvable
  by any available algorithm.

  The second strand of this work is concerned with synthetic data generation.
  Testing algorithms across a wide range of problem instances is crucial to
  ensure the validity of any claim about one algorithm's superiority over
  another. However, benchmarks are often limited and fail to reveal differences
  among the algorithms. First, we show how random instances of probabilistic
  logic programs (that typically use WMC algorithms for inference) can be
  generated using constraint programming. We also introduce a new constraint to
  control the independence structure of the underlying probability distribution
  and provide a combinatorial argument for the correctness of the constraint
  model. This model allows us to, for the first time, experimentally investigate
  inference algorithms on more than just a handful of instances. Second, we
  introduce a random model for WMC instances with a parameter that influences
  primal treewidth---the parameter most commonly used to characterise the
  difficulty of an instance. We show that the easy-hard-easy pattern with
  respect to clause density is different for algorithms based on dynamic
  programming and algebraic decision diagrams than for all other solvers. We
  also demonstrate that all WMC algorithms scale exponentially with respect to
  primal treewidth, although at differing rates.}

\begin{document}
\nobibliography*

\begin{preliminary}

\maketitle

\begin{laysummary}
  Given a task to compute a number (e.g., some probability), not all approaches
  are equally efficient. For example, multiplying two integers is likely to be
  faster than simulating multiplication via repeated addition. Moreover, this
  computational task is given to us in some kind of format, and this format
  determines whether an efficient solution is easy to find. Weighted model
  counting (WMC) is an approach to computing sums of products efficiently by
  using logic to describe what needs to be computed. WMC is heavily used in
  artificial intelligence, machine learning, robotics, and many other fields.

  This thesis approaches WMC from two fronts. First, we focus on improving WMC
  algorithms by generalising some of the key ideas and making them more flexible
  and powerful. To this end, we revisit the foundations of WMC and develop new
  input formats that connect logic with numbers (which need to be added and/or
  multiplied) in a more flexible way. We also improve the ability of a WMC
  algorithm to solve subproblems by recognizing them as variations of
  subproblems encountered before. Second, we improve our understanding of the
  differences among WMC algorithms by generating a variety of problem instances
  and testing the algorithms on them. For instances written in a more complex
  format, we find that all WMC algorithms behave extremely similarly, as the
  computational bottleneck seems to happen before WMC even takes place. On the
  other hand, for instances written in a simpler format, we show important
  differences in the performance characteristics of WMC algorithms depending on
  several key parameters that are used to describe these instances.
\end{laysummary}

\begin{acknowledgements}
  First and foremost, I would like to thank the anonymous reviewers of various
  AI conferences for their expertise and guidance. Reviewer feedback helped me
  tremendously in aligning my research with the questions and concerns germane
  to the broader community.

  I would also like to thank my principal supervisor Vaishak Belle for giving me
  the freedom to explore whatever ideas fascinated me at the time. I am also
  grateful to him for regularly reminding me of relevant conference deadlines
  and encouraging me to submit my work early and often.

  I would like to thank my assistant supervisor Ron Petrick as well as Alan
  Bundy for participating in my annual reviews and helping me better understand
  my strengths and weaknesses. Similarly, I am grateful to my examiners Guy Van
  den Broeck and James Cheney for agreeing to spend their time examining my
  work.

  I wish to thank Ciaran McCreesh for (together with Patrick Prosser) providing
  me with my very first introduction to the world of research and staying in
  touch over subsequent years. I am also grateful to him for inviting me to
  (virtually) revisit my alma mater and give a talk there.

  % Ciaran McCreesh for invitation to speak, pre-PhD mentorship, staying in touch

  Despite all events being virtual for most of my PhD, I still got to meet many
  researchers from other institutions. I appreciate the thoughtful conversations
  I have had with Adnan Darwiche, Jeffrey Dudek, Timothy van Bremen, and others
  I have met at conferences and workshops.

  My PhD experience has been greatly enriched by other PhD students at the
  University of Edinburgh. In particular, I am grateful to Fazl, Gini, Imogen,
  Jonathan, and S\'{a}ndor for the opportunities to learn about related (or
  otherwise interesting) lines of research. More broadly, I have greatly enjoyed
  the company of the members of the Artificial Intelligence Applications
  Institute and the Edinburgh Centre for Robotics. Finally, my PhD would not
  have been possible without the prompt replies from the support staff at the
  aforementioned institutions.

  This work was supported by the EPSRC Centre for Doctoral Training in Robotics
  and Autonomous Systems, funded by the UK Engineering and Physical Sciences
  Research Council (grant EP/L016834/1). Furthermore, this work has made use of
  the resources provided by the Edinburgh Compute and Data Facility (ECDF;
  \url{www.ecdf.ed.ac.uk}).
\end{acknowledgements}

\standarddeclaration

%% Finally, a dedication (this is optional -- uncomment the following line if
%% you want one).
% \dedication{To my mummy.}

\tableofcontents

%% If you want a list of figures or tables, uncomment the appropriate line(s)
% \listoffigures
% \listoftables

\end{preliminary}

% TODO:
% * TODOs in background and conclusion
% * introduction: define / write a bit about pseudo-Boolean functions (necessary for the next chapter).
% * use of input/output vs data/result in algorithms
% * run everything through a grammar checker

\include{chapters/introduction} % 5-14 pages (9 on average)
\include{chapters/background} % 11-45 pages (27 on average)
\include{chapters/wmc_for_bns/chapter}
\include{chapters/wmc_without_parameters/chapter}
\include{chapters/recursion}
\include{chapters/random_lps/chapter}
\include{chapters/comparison/chapter}
\include{chapters/conclusion} % 5-28 pages (mean: 8, median: 5)

%% If you want the bibliography single-spaced (which is allowed), uncomment
%% the next line.
\singlespace

\bibliography{thesis}
%\printbibliography

\end{document}
